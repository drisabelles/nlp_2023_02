{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exercícios: Pré-processamento\nFonte: CARVALHO, Fabrício Galende Marques de. **Notas de aula da disciplina processamento de linguagem natural.** São José dos Campos, 2023.\n\n#### Atividade realizada em grupo: \n- Gabriel Camargo Leite\n- Giovana Thaís de O. Silva\n- Isabelle Dias R. Silva\n- João Marcos O. Santos\n- Maria Gabriela G. S. Reis\n- Thiago Henrique Ferreira","metadata":{}},{"cell_type":"markdown","source":"## Terminologia e conceitos\n---\n\n1.  (TC.2.2) Qual um possível efeito da não remoção de um iframe ou de scripts e CSS’s de um documento capturado através de uma raspagem de dados? \n\n**Resposta:** Conforme consultado no material de aula, caso a remoção de tags, executada depois da raspagem de dados em rede (web scraping), não seja feita, ou seja, se as tags e dados não informativos (scripts, css, iframes…) não forem removidos do texto, é possível que ruídos sejam criados na base de dados a ser utilizada pelas próximas etapas da pipeline, o que poderia influenciar em análises errôneas.\n\n2. (TC.2.4) Cite exemplos de tokens de frases e tokens de palavras que podem significar:\n\n| Situação                                                                          | Token de frase             | Token de palavra |\n|:----------------------------------------------------------------------------------|:---------------------------|:-----------------|\n| Opinião negativa referente a um produto de uma loja de vestuário                  | Tecido ruim                | Largo            |\n| Opinião positiva referente a um produto de uma loja de vestuário                  | Costura muito boa          | Confortável      |\n| Opinião neutra referente a um produto de uma loja de vestuário                    | Veste mais ou menos        | OK               |\n| Opinião negativa relacionada a um carro vendido por uma concessionária automotiva | Câmbio impreciso           | Desalinhado      |\n| Opinião positiva relacionada a um carro vendido por uma concessionária automotiva | Manutenção simples         | Robustez         |\n| Opinião neutra relacionada a um carro vendido por uma concessionária automotiva   | Carro bom, mas gasta muito | Decente          |\n","metadata":{}},{"cell_type":"markdown","source":"## Prática de programação\n---\n\n1. (PP.2.1) Tomando como base o código-fonte fornecido pelo professor e efetuando uma raspagem de dados que opere sobre alguma página contendo revisão de produtos, efetue uma comparação de desempenho utilizando 3 modelos de tokenizadores de sentença. Qual sua conclusão?\n\n> **OBS:** Kaggle talvez pode não deixa pegar a review do Mercado Livre (proteção contra requisições)","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# Instalação do pacote em Português do Spacy\n# =============================================================================\n!python -m spacy download pt","metadata":{"execution":{"iopub.status.busy":"2023-09-26T00:26:41.351876Z","iopub.execute_input":"2023-09-26T00:26:41.352219Z","iopub.status.idle":"2023-09-26T00:26:58.127012Z","shell.execute_reply.started":"2023-09-26T00:26:41.352193Z","shell.execute_reply":"2023-09-26T00:26:58.125464Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'pt' are deprecated. Please use the\nfull pipeline package name 'pt_core_news_sm' instead.\u001b[0m\nCollecting pt-core-news-sm==3.6.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.6.0/pt_core_news_sm-3.6.0-py3-none-any.whl (13.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /opt/conda/lib/python3.10/site-packages (from pt-core-news-sm==3.6.0) (3.6.0)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (1.0.4)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (1.0.9)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2.0.7)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (3.0.8)\nRequirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (8.1.10)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2.4.6)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2.0.8)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (0.9.0)\nRequirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (0.10.2)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (6.3.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (4.65.0)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (1.23.5)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (1.10.9)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (59.8.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (3.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (3.0.9)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (4.6.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2023.5.7)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (0.7.9)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (0.1.0)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (8.1.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2.1.3)\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('pt_core_news_sm')\n","output_type":"stream"}]},{"cell_type":"code","source":"from colorama import Fore, Back, Style \nimport requests\nfrom bs4 import BeautifulSoup\nimport time\nfrom transformers import AutoTokenizer\nimport nltk\n#nltk.download('punkt')\nimport spacy\nnlp = spacy.load(\"pt_core_news_sm\")\n\ndef format_time(seconds):\n    if seconds < 1:\n        return f\"{seconds * 1000:.2f} ms\"\n    else:\n        return f\"{seconds:.2f} s\"\n    \ndef spacy_tokenizer(text):\n    doc = nlp(text)\n    tokens = [token.text for token in doc]\n    return tokens\n\n# a function to get rid of html tags\ndef get_rid_html_tags(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    # iframe and script nodes removal from doc tree\n    [s.extract() for s in soup(['iframe','script'])]\n    stripped_text = soup.get_text()\n    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]','\\n',stripped_text)\n    return stripped_text\n\ndef cus_data(soup):\n    # find the Html tag with find() and convert into string\n    data_str = \"\"\n    cus_list = []\n  \n    for item in soup.find_all(\"p\", class_=\"ui-review-capability-comments__comment__content\"):\n        data_str = data_str + item.get_text()\n        cus_list.append(data_str)\n        data_str = \"\"\n    return cus_list\n\n\ndata = requests.get(\"https://www.mercadolivre.com.br/controle-joystick-sem-fio-sony-playstation-dualsense-edge-branco/p/MLB21286427?pdp_filters=category:MLB455266#reviews\")\n\ncontent = data.content\nsoup = BeautifulSoup(content, \"html.parser\")\n\ncus_res = cus_data(soup)\nprint(\"\\n=============================\\nAvaliações do controle de PS5\\n=============================\\n\")\n\nfor y in range(0, len(cus_res)):\n    print(f'{y+1}: {cus_res[y]}')\n    review_text = cus_res[y]\n    \n    # NLTK_TOKENIZER\n    start_time = time.time()\n    nltk_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    default_nltk_tokens = nltk_tokenizer.tokenize(review_text)\n    nltk_elapsed_time = time.time() - start_time\n    print(\"=================================================================\")\n\n    print(f'Tokens {Fore.BLUE}NLTK (padrão){Fore.RESET}: {default_nltk_tokens}')\n    print(f'{Fore.GREEN} Tempo de processamento {Fore.BLUE}NLTK (padrão){Fore.GREEN}: {format_time(nltk_elapsed_time)} {Fore.RESET}')\n    print(\"- - - - - - - - - - -\")\n\n    # Tokenizador simples (espaço como delimitador)\n    start_time = time.time()\n    simple_tokens = review_text.split()  # Usando split() diretamente\n    simple_elapsed_time = time.time() - start_time\n\n    print(f'Tokens com {Fore.BLUE}delimitador de espaço{Fore.RESET}: {simple_tokens}')\n    print(f'{Fore.GREEN} Tempo de processamento com {Fore.BLUE} delimitador de espaço {Fore.GREEN}: {format_time(simple_elapsed_time)} {Fore.RESET}')\n    print(\"- - - - - - - - - - -\")\n    \n    # Medindo o desempenho do tokenizador spaCy\n    start_time = time.time()\n    spacy_tokens = spacy_tokenizer(review_text)\n    spacy_elapsed_time = time.time() - start_time\n    \n    print(f'Tokens {Fore.BLUE}spaCy{Fore.RESET}: {spacy_tokens}')\n    print(f'{Fore.GREEN} Tempo de processamento {Fore.BLUE}spaCy{Fore.GREEN}: {format_time(spacy_elapsed_time)} {Fore.RESET}')\n    print(\"- - - - - - - - - - -\")\n    \n    # Medindo o desempenho do tokenizador HuggingFace Transform\n    start_time = time.time()\n    transformers_tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n    hugging_tokens = transformers_tokenizer.tokenize(review_text)\n    hugging_elapsed_time = time.time() - start_time\n\n    print(f'Tokens {Fore.BLUE}HuggingFace{Fore.RESET}: {hugging_tokens}')\n    print(f'{Fore.GREEN} Tempo de processamento {Fore.BLUE}HuggingFace{Fore.GREEN}: {format_time(hugging_elapsed_time)} {Fore.RESET}') \n    \n    print(\"=================================================================\\n\\n\\n\\n\")\n    \nprint(\"\\n************ END ************\")","metadata":{"execution":{"iopub.status.busy":"2023-09-26T00:27:01.978288Z","iopub.execute_input":"2023-09-26T00:27:01.978676Z","iopub.status.idle":"2023-09-26T00:27:05.766980Z","shell.execute_reply.started":"2023-09-26T00:27:01.978645Z","shell.execute_reply":"2023-09-26T00:27:05.766002Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"\n=============================\nAvaliações do controle de PS5\n=============================\n\n1: Excelente, top ,show de bola.\n=================================================================\nTokens \u001b[34mNLTK (padrão)\u001b[39m: ['Excelente', 'top', 'show', 'de', 'bola']\n\u001b[32m Tempo de processamento \u001b[34mNLTK (padrão)\u001b[32m: 0.22 ms \u001b[39m\n- - - - - - - - - - -\nTokens com \u001b[34mdelimitador de espaço\u001b[39m: ['Excelente,', 'top', ',show', 'de', 'bola.']\n\u001b[32m Tempo de processamento com \u001b[34m delimitador de espaço \u001b[32m: 0.00 ms \u001b[39m\n- - - - - - - - - - -\nTokens \u001b[34mspaCy\u001b[39m: ['Excelente', ',', 'top', ',', 'show', 'de', 'bola', '.']\n\u001b[32m Tempo de processamento \u001b[34mspaCy\u001b[32m: 23.33 ms \u001b[39m\n- - - - - - - - - - -\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/43.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee757ced025a40909c0925ae9c1fc8d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/647 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"814398090b8a46779b4759b961e1b297"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/210k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08d260327b5a43b0b2f5c8fad382a8eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)in/added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22fb784ff5da4006a79006da9e1ffeaa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"629a738d5b264dfdaab073f170fdb06f"}},"metadata":{}},{"name":"stdout","text":"Tokens \u001b[34mHuggingFace\u001b[39m: ['Excel', '##ente', ',', 'top', ',', 'show', 'de', 'bola', '.']\n\u001b[32m Tempo de processamento \u001b[34mHuggingFace\u001b[32m: 1.29 s \u001b[39m\n=================================================================\n\n\n\n\n2: Excelente.\n=================================================================\nTokens \u001b[34mNLTK (padrão)\u001b[39m: ['Excelente']\n\u001b[32m Tempo de processamento \u001b[34mNLTK (padrão)\u001b[32m: 0.04 ms \u001b[39m\n- - - - - - - - - - -\nTokens com \u001b[34mdelimitador de espaço\u001b[39m: ['Excelente.']\n\u001b[32m Tempo de processamento com \u001b[34m delimitador de espaço \u001b[32m: 0.00 ms \u001b[39m\n- - - - - - - - - - -\nTokens \u001b[34mspaCy\u001b[39m: ['Excelente', '.']\n\u001b[32m Tempo de processamento \u001b[34mspaCy\u001b[32m: 5.61 ms \u001b[39m\n- - - - - - - - - - -\nTokens \u001b[34mHuggingFace\u001b[39m: ['Excel', '##ente', '.']\n\u001b[32m Tempo de processamento \u001b[34mHuggingFace\u001b[32m: 154.97 ms \u001b[39m\n=================================================================\n\n\n\n\n3: Controle de qualidade e melhora bastante a jogatina no fps. Depois que se acostuma realmente faz diferença!.\n=================================================================\nTokens \u001b[34mNLTK (padrão)\u001b[39m: ['Controle', 'de', 'qualidade', 'e', 'melhora', 'bastante', 'a', 'jogatina', 'no', 'fps', 'Depois', 'que', 'se', 'acostuma', 'realmente', 'faz', 'diferença']\n\u001b[32m Tempo de processamento \u001b[34mNLTK (padrão)\u001b[32m: 0.04 ms \u001b[39m\n- - - - - - - - - - -\nTokens com \u001b[34mdelimitador de espaço\u001b[39m: ['Controle', 'de', 'qualidade', 'e', 'melhora', 'bastante', 'a', 'jogatina', 'no', 'fps.', 'Depois', 'que', 'se', 'acostuma', 'realmente', 'faz', 'diferença!.']\n\u001b[32m Tempo de processamento com \u001b[34m delimitador de espaço \u001b[32m: 0.01 ms \u001b[39m\n- - - - - - - - - - -\nTokens \u001b[34mspaCy\u001b[39m: ['Controle', 'de', 'qualidade', 'e', 'melhora', 'bastante', 'a', 'jogatina', 'no', 'fps', '.', 'Depois', 'que', 'se', 'acostuma', 'realmente', 'faz', 'diferença', '!', '.']\n\u001b[32m Tempo de processamento \u001b[34mspaCy\u001b[32m: 7.66 ms \u001b[39m\n- - - - - - - - - - -\nTokens \u001b[34mHuggingFace\u001b[39m: ['Controle', 'de', 'qualidade', 'e', 'melhora', 'bastante', 'a', 'joga', '##tina', 'no', 'f', '##ps', '.', 'Depois', 'que', 'se', 'aco', '##st', '##uma', 'realmente', 'faz', 'diferença', '!', '.']\n\u001b[32m Tempo de processamento \u001b[34mHuggingFace\u001b[32m: 151.72 ms \u001b[39m\n=================================================================\n\n\n\n\n\n************ END ************\n","output_type":"stream"}]},{"cell_type":"markdown","source":"2. (PP.2.4) Exemplifique o funcionamento de um token de palavras utilizando expressão regular. Explique, com um programa exemplo, como configurar o padrão para obter um comportamento diferente do tokenizador. ","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import regexp_tokenize\n\nfrase = 'Tornar o futuro seguro e Próspero, conectando ciência e Tecnologia!'\n\n# Frase retirando espaços\ntokenizacao1 = regexp_tokenize(frase, r'\\s+', gaps=True)\n\n# Frase selecionando caracter maiúsculo \ntokenizacao2 = regexp_tokenize(frase, r'[A-Z]\\w+')\n\nprint(tokenizacao1)\nprint('---------------------------------------------------------')\nprint(tokenizacao2)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T00:27:31.399974Z","iopub.execute_input":"2023-09-26T00:27:31.400375Z","iopub.status.idle":"2023-09-26T00:27:31.408291Z","shell.execute_reply.started":"2023-09-26T00:27:31.400343Z","shell.execute_reply":"2023-09-26T00:27:31.406964Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"['Tornar', 'o', 'futuro', 'seguro', 'e', 'Próspero,', 'conectando', 'ciência', 'e', 'Tecnologia!']\n---------------------------------------------------------\n['Tornar', 'Próspero', 'Tecnologia']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"3. (PP.2.6) Exemplifique o funcionamento de um corretor ortográfico, aplicável à língua portuguesa, que efetue correção de palavras baseado em um corpus de texto considerado como referência e que utilize métricas de distância e estatísticas de ocorrência de palavras no corpus considerado. Alterar o corpus pode afetar o comportamento do corretor? Se sim, dê um exemplo prático utilizando dados diferentes para o corpus.","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-08-29T01:27:23.423042Z","iopub.execute_input":"2023-08-29T01:27:23.423373Z","iopub.status.idle":"2023-08-29T01:27:24.201888Z","shell.execute_reply.started":"2023-08-29T01:27:23.423339Z","shell.execute_reply":"2023-08-29T01:27:24.200782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. (PP.2.7) Aplique técnicas de tokenização e correção de erros de ortografia a dados de revisão de produtos que tenham sido raspados de uma página de revisão da Internet. Ilustre o comportamento e o desempenho do seu trecho de pipeline de PLN identificando os principais gargalos e sugira uma melhoria possível. Esclareça o porquê da ordem dos elementos em sua pipeline.","metadata":{}},{"cell_type":"code","source":"# Importando bibliotecas\nimport pandas as pd\nimport nltk\nfrom nltk import tokenize    \nfrom nltk.corpus import words\nfrom nltk.metrics.distance import edit_distance\n\n# Definindo frase completa\nfrase = \"Good resolutionn quality hoewever the shape of the packging left somethinggg to be desirred\"\n\n# Tokenizando a frase\ntokens = tokenize.word_tokenize(frase)\nprint(tokens)\nprint('----------------------------------------------------------------')\n\n# Importando palavras corretas\npalavras_corretas = words.words()\nprint(palavras_corretas[:20])\nprint('----------------------------------------------------------------')\n\ndata = []\n\nfor token in tokens:\n    menorDistancia = {\"distancia\": 100, \"palavra\": \"\"}\n    \n    for palavra in palavras_corretas:\n        distancia = edit_distance(token, palavra)\n        if distancia < menorDistancia[\"distancia\"]:\n            menorDistancia[\"distancia\"] = distancia\n            menorDistancia[\"palavra\"] = palavra\n   \n    palavra_corrigida = [menorDistancia[\"palavra\"], menorDistancia[\"distancia\"]]\n    data.append(palavra_corrigida)\n\nprint(data)\nprint('----------------------------------------------------------------')\n\n# for token in tokens:\n#     temp = [(edit_distance(token, palavra), palavra) for palavra in palavras_corretas if palavra[0]==token[0]]\n#     print(sorted(temp, key = lambda val:val[0])[0][1])\n\ndf = pd.DataFrame(data,columns=['Palavra corrigida','Distância'])\ndf.head(50)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T00:27:34.232183Z","iopub.execute_input":"2023-09-26T00:27:34.232589Z","iopub.status.idle":"2023-09-26T00:30:42.955300Z","shell.execute_reply.started":"2023-09-26T00:27:34.232557Z","shell.execute_reply":"2023-09-26T00:30:42.954027Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"['Good', 'resolutionn', 'quality', 'hoewever', 'the', 'shape', 'of', 'the', 'packging', 'left', 'somethinggg', 'to', 'be', 'desirred']\n----------------------------------------------------------------\n['A', 'a', 'aa', 'aal', 'aalii', 'aam', 'Aani', 'aardvark', 'aardwolf', 'Aaron', 'Aaronic', 'Aaronical', 'Aaronite', 'Aaronitic', 'Aaru', 'Ab', 'aba', 'Ababdeh', 'Ababua', 'abac']\n----------------------------------------------------------------\n[['bood', 1], ['resolution', 1], ['quality', 0], ['however', 1], ['the', 0], ['shape', 0], ['of', 0], ['the', 0], ['backing', 2], ['left', 0], ['something', 2], ['to', 0], ['be', 0], ['desired', 1]]\n----------------------------------------------------------------\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"   Palavra corrigida  Distância\n0               bood          1\n1         resolution          1\n2            quality          0\n3            however          1\n4                the          0\n5              shape          0\n6                 of          0\n7                the          0\n8            backing          2\n9               left          0\n10         something          2\n11                to          0\n12                be          0\n13           desired          1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Palavra corrigida</th>\n      <th>Distância</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>bood</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>resolution</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>quality</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>however</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>the</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>shape</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>of</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>the</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>backing</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>left</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>something</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>to</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>be</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>desired</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}